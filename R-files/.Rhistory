grouping = ifelse(
as.character(distractor_with_target) == as.character(IA2),
"together",
"apart"
)
)
# aggregate data by subjects
dist_by_subj <- aggregate_data(
target_group_long_data,
"subject",
"grouping",
"fix"
)
# aggregate by items
dist_by_item <- aggregate_data(
target_group_long_data,
"item",
"grouping",
"fix"
)
# establish centering list
target_group_centering_list <- list(
factors = c("grouping", "IA2"),
levels = c("together", "d1")
)
# make new dvs
dist_by_subj <- make_dvs(dist_by_subj, "y", "N")
dist_by_item <- make_dvs(dist_by_item, "y", "N")
# interaction model: testing for main effects and interactions
dist_by_subj_model <- tidy_model(
lmer(asin ~ grouping + (1 | by), data = dist_by_subj)
)
dist_by_item_model <- tidy_model(
lmer(asin ~ grouping + (1 | by), data = dist_by_item)
)
merge_tables(dist_by_subj_model, dist_by_item_model) %>%
pretty_confint(., "2.5 %", "97.5 %") %>%
kable()
dist_by_subj %>%
group_by(grouping) %>%
summarise(
n = length(unique(by)),
prop_mean = mean(prop),
prop_sd = sd(prop),
prop_ci = ci_of_mean(prop_mean, prop_sd, n),
asin_mean = mean(asin),
asin_sd = sd(asin),
asin_ci = ci_of_mean(asin_mean, asin_sd, n)
) %>%
select(-n) %>%
kable()
dist_bf <- data.frame(
model = c("by_subj", "by_items"),
BF_01 = vector("numeric", 2)
)
# by-subjects
dist_maximal_subj <- lmer(asin ~ grouping + (1 | by), data = dist_by_subj)
dist_reduced_subj <- lmer(asin ~ 1 + (1 | by), data = dist_by_subj)
dist_bf$BF_01[1] <- exp((BIC(dist_maximal_subj) - BIC(dist_reduced_subj)) / 2)
# by-items
dist_maximal_items <- lmer(asin ~ grouping + (1 | by), data = dist_by_item)
dist_reduced_items <- lmer(asin ~ 1 + (1 | by), data = dist_by_item)
dist_bf$BF_01[2] <- exp((BIC(dist_maximal_items) - BIC(dist_reduced_items)) / 2)
# load packages and helper functions
required_packages <- c(
"lme4",
"multcomp",
"dplyr",
"tidyr",
"stringr",
"broom",
"ggplot2",
"knitr",
"rmarkdown"
)
lapply(required_packages, library, character.only = TRUE)
source("01_helper-functions.R")
filenames <- c(
"02_semantic-results.Rmd",
"03_visual-results.Rmd",
"04_additional-analyses.Rmd"
)
# render documents
for (i in seq_along(filenames)) {
render(filenames[i], output_dir = "../output/")
}
options(scipen = 1, digits = 3)
load("../data/semantic_data.Rdata")
semantic_demo <- read.csv("../data/semantic_demographics.csv")
semantic_item_checks <- read.csv("../data/semantic_item_checks.csv")
options(scipen = 1, digits = 3)
load("../data/semantic_data.Rdata")
semantic_demo <- read.csv("../data/semantic_demographics.csv")
semantic_item_checks <- read.csv("../data/semantic_item_checks.csv")
subject_lists <- merge(
filter(semantic_demo, included == "yes"), semantic_item_checks) %>%
dplyr::select(c(
subject,
item,
list,
condition,
objects_between_mention,
distractor_with_target
)) %>%
mutate(
subject = as.factor(str_pad(subject, 2, pad = "0")),
item = as.factor(item)
)
subject_lists <- merge(
filter(semantic_demo, included == "yes"), semantic_item_checks) %>%
dplyr::select(c(
subject,
item,
list,
objects_between_mention,
distractor_with_target
)) %>%
mutate(
subject = as.factor(str_pad(subject, 2, pad = "0")),
item = as.factor(item)
)
head(subject_lists)
head(semantic_item_checks)
head(semantic_demo)
head(semantic_item_checks)
subject_lists <- merge(
filter(semantic_demo, included == "yes"), semantic_item_checks) %>%
dplyr::select(c(
subject,
item,
list,
objects_between_mention,
distractor_with_target
)) %>%
mutate(
subject = as.factor(str_pad(subject, 2, pad = "0")),
item = as.factor(item)
)
# define time window
t_window <- c("crit_noun_on", "crit_noun_off")
# shift window by how much?
window_shift <- 300
# tidy data before analysis, restricting to the competitor only
prox_tidy_data <- semantic_data %>%
mutate(
item = as.factor(item),
time_0 = time - UQ(as.name(t_window[1]))
) %>%
dplyr::select(-c(t, d1, d2))
# subset data to time window (already in long format)
prox_sub_data <- subset_to_window(
prox_tidy_data,
"time_0",
timeM,
t_window,
window_shift
) %>%
rename(fix = c)
prox_long_data <- inner_join(
prox_sub_data,
subject_lists,
by = c("subject", "item", "condition")
) %>%
rename(prox = objects_between_mention) %>%
mutate(prox = as.factor(prox))
prox_long_data <- inner_join(
prox_sub_data,
subject_lists,
by = c("subject", "item")
) %>%
rename(prox = objects_between_mention) %>%
mutate(prox = as.factor(prox))
prox_by_subj <- aggregate_data(prox_long_data, "subject", "prox", "fix")
prox_by_subj <- make_dvs(prox_by_subj, "y", "N")
# aggregate data by items, center variables, and make new dvs
prox_by_item <- aggregate_data(prox_long_data, "item", "prox", "fix")
prox_by_item <- make_dvs(prox_by_item, "y", "N")
# main_model_formula
prox_main_formula <- as.formula("asin ~ prox + (1 | by)")
# interaction model: testing for main effects and interactions
prox_by_subj_model <- lmer(prox_main_formula, data = prox_by_subj)
prox_by_item_model <- lmer(prox_main_formula, data = prox_by_item)
# establish contrast matrix of planned comparisons
contrast_matrix <- rbind(
"0 vs. 1" = c(0, 1, 0),
"0 vs. 2" = c(0, 0, 1),
"2 vs. 3" = c(0, -1, 1)
)
# fit contrasts and manually adjust p-values
# for all tests (i.e. accounting for doing split by-subject/item analysis)
prox_by_subj_comparisons <- test_many_levels(prox_by_subj_model, contrast_matrix)
prox_by_item_comparisons <- test_many_levels(prox_by_item_model, contrast_matrix)
merge_tables(prox_by_subj_comparisons, prox_by_item_comparisons) %>%
pretty_confint(., "conf.low", "conf.high") %>%
rename(p_value = p.value) %>%
kable()
prox_by_subj %>%
group_by(prox) %>%
summarise(
n = length(unique(by)),
prop_mean = mean(prop),
prop_sd = sd(prop),
prop_ci = ci_of_mean(prop_mean, prop_sd, n),
asin_mean = mean(asin),
asin_sd = sd(asin),
asin_ci = ci_of_mean(asin_mean, asin_sd, n)
) %>%
select(-n) %>%
rename(proximity = prox) %>%
kable()
options(scipen = 1, digits = 3)
load("../data/semantic_data.Rdata")
semantic_demo <- read.csv("../data/semantic_demographics.csv")
semantic_item_checks <- read.csv("../data/semantic_item_checks.csv")
subject_lists <- merge(
filter(semantic_demo, included == "yes"), semantic_item_checks) %>%
dplyr::select(c(
subject,
item,
list,
objects_between_mention,
distractor_with_target
)) %>%
mutate(
subject = as.factor(str_pad(subject, 2, pad = "0")),
item = as.factor(item)
)
# define time window
t_window <- c("crit_noun_on", "crit_noun_off")
# shift window by how much?
window_shift <- 300
# tidy data before analysis, restricting to the competitor only
prox_tidy_data <- semantic_data %>%
mutate(
item = as.factor(item),
time_0 = time - UQ(as.name(t_window[1]))
) %>%
dplyr::select(-c(t, d1, d2))
# subset data to time window (already in long format)
prox_sub_data <- subset_to_window(
prox_tidy_data,
"time_0",
timeM,
t_window,
window_shift
) %>%
rename(fix = c)
# join data sets, dropping together conditions
prox_long_data <- inner_join(
prox_sub_data,
subject_lists,
by = c("subject", "item")
) %>%
rename(prox = objects_between_mention) %>%
mutate(prox = as.factor(prox))
# aggregate data by subjects, center variables, and make new dvs
prox_by_subj <- aggregate_data(prox_long_data, "subject", "prox", "fix")
prox_by_subj <- make_dvs(prox_by_subj, "y", "N")
# aggregate data by items, center variables, and make new dvs
prox_by_item <- aggregate_data(prox_long_data, "item", "prox", "fix")
prox_by_item <- make_dvs(prox_by_item, "y", "N")
# main_model_formula
prox_main_formula <- as.formula("asin ~ prox + (1 | by)")
# interaction model: testing for main effects and interactions
prox_by_subj_model <- lmer(prox_main_formula, data = prox_by_subj)
prox_by_item_model <- lmer(prox_main_formula, data = prox_by_item)
# establish contrast matrix of planned comparisons
contrast_matrix <- rbind(
"0 vs. 1" = c(0, 1, 0),
"0 vs. 2" = c(0, 0, 1),
"2 vs. 3" = c(0, -1, 1)
)
# fit contrasts and manually adjust p-values
# for all tests (i.e. accounting for doing split by-subject/item analysis)
prox_by_subj_comparisons <- test_many_levels(prox_by_subj_model, contrast_matrix)
prox_by_item_comparisons <- test_many_levels(prox_by_item_model, contrast_matrix)
merge_tables(prox_by_subj_comparisons, prox_by_item_comparisons) %>%
pretty_confint(., "conf.low", "conf.high") %>%
rename(p_value = p.value) %>%
kable()
prox_by_subj %>%
group_by(prox) %>%
summarise(
n = length(unique(by)),
prop_mean = mean(prop),
prop_sd = sd(prop),
prop_ci = ci_of_mean(prop_mean, prop_sd, n),
asin_mean = mean(asin),
asin_sd = sd(asin),
asin_ci = ci_of_mean(asin_mean, asin_sd, n)
) %>%
select(-n) %>%
rename(proximity = prox) %>%
kable()
prox_data_list <- list(
subj_zero_vs_one = prox_by_subj %>% filter(prox != 2),
subj_zero_vs_two = prox_by_subj %>% filter(prox != 1),
subj_one_vs_two = prox_by_subj %>% filter(prox != 0),
item_zero_vs_one = prox_by_item %>% filter(prox != 2),
item_zero_vs_two = prox_by_item %>% filter(prox != 1),
item_one_vs_two = prox_by_item %>% filter(prox != 0)
)
prox_bf <- data.frame(
model = names(prox_data_list),
BIC_H0 = vector("numeric", 6),
BIC_H1 = vector("numeric", 6),
BF_01 = vector("numeric", 6)
)
for (i in seq_along(names(prox_data_list))) {
if(names(prox_data_list)[i] %>% stringr::str_detect("subj")) {
maximal <- lmer(asin ~ prox + (1 | by), data = prox_data_list[[i]])
reduced <- lmer(asin ~ 1 + (1 | by), data = prox_data_list[[i]])
} else {
maximal <- lm(asin ~ prox, data = prox_data_list[[i]])
reduced <- lm(asin ~ 1, data = prox_data_list[[i]])
}
# save output
prox_bf$BIC_H0[i] <- BIC(reduced)
prox_bf$BIC_H1[i] <- BIC(maximal)
prox_bf$BF_01[i] <- exp((BIC(maximal) - BIC(reduced)) / 2)
}
prox_bf %>%
mutate(
aggregate = c(rep("Subjects", 3), rep("Items", 3)),
model = rep(c("Zero vs. One", "Zero vs. Two", "One vs. Two"), 2)
) %>%
rename(
group = model,
"*BF~01~*" = BF_01,
"*BIC*(H~0~)" = BIC_H0,
"*BIC*(H~1~)" = BIC_H1
) %>%
select(aggregate, everything()) %>%
kable(escape = FALSE)
# reset interest area labels for later subsetting
target_group_ias <- c("d1", "d2")
# tidy data before analysis (make distractor looks the average across the two)
target_group_tidy_data <- semantic_data %>%
mutate(
item = as.factor(item),
time_0 = time - UQ(as.name(t_window[1]))
) %>%
dplyr::select(-t)
# subset data to time window
target_group_sub_data <- subset_to_window(
target_group_tidy_data,
"time_0",
timeM,
t_window,
window_shift
)
target_group_long_data <- make_data_long(
target_group_sub_data,
"IA2",
target_group_ias,
"fix"
)
# merge with information about subject conditions
# keep only apart conditions where these checks are appropriate
target_group_long_data <- inner_join(
target_group_long_data,
subject_lists,
by = c("subject", "item", "condition")
) %>%
rename(prox = objects_between_mention) %>%
mutate(
prox = as.factor(prox),
grouping = ifelse(
as.character(distractor_with_target) == as.character(IA2),
"together",
"apart"
)
)
# reset interest area labels for later subsetting
target_group_ias <- c("d1", "d2")
# tidy data before analysis (make distractor looks the average across the two)
target_group_tidy_data <- semantic_data %>%
mutate(
item = as.factor(item),
time_0 = time - UQ(as.name(t_window[1]))
) %>%
dplyr::select(-t)
# subset data to time window
target_group_sub_data <- subset_to_window(
target_group_tidy_data,
"time_0",
timeM,
t_window,
window_shift
)
target_group_long_data <- make_data_long(
target_group_sub_data,
"IA2",
target_group_ias,
"fix"
)
# merge with information about subject conditions
# keep only apart conditions where these checks are appropriate
target_group_long_data <- inner_join(
target_group_long_data,
subject_lists,
by = c("subject", "item")
) %>%
rename(prox = objects_between_mention) %>%
mutate(
prox = as.factor(prox),
grouping = ifelse(
as.character(distractor_with_target) == as.character(IA2),
"together",
"apart"
)
)
# aggregate data by subjects
dist_by_subj <- aggregate_data(
target_group_long_data,
"subject",
"grouping",
"fix"
)
# aggregate by items
dist_by_item <- aggregate_data(
target_group_long_data,
"item",
"grouping",
"fix"
)
# establish centering list
target_group_centering_list <- list(
factors = c("grouping", "IA2"),
levels = c("together", "d1")
)
# make new dvs
dist_by_subj <- make_dvs(dist_by_subj, "y", "N")
dist_by_item <- make_dvs(dist_by_item, "y", "N")
# interaction model: testing for main effects and interactions
dist_by_subj_model <- tidy_model(
lmer(asin ~ grouping + (1 | by), data = dist_by_subj)
)
dist_by_item_model <- tidy_model(
lmer(asin ~ grouping + (1 | by), data = dist_by_item)
)
merge_tables(dist_by_subj_model, dist_by_item_model) %>%
pretty_confint(., "2.5 %", "97.5 %") %>%
kable()
dist_by_subj %>%
group_by(grouping) %>%
summarise(
n = length(unique(by)),
prop_mean = mean(prop),
prop_sd = sd(prop),
prop_ci = ci_of_mean(prop_mean, prop_sd, n),
asin_mean = mean(asin),
asin_sd = sd(asin),
asin_ci = ci_of_mean(asin_mean, asin_sd, n)
) %>%
select(-n) %>%
kable()
dist_bf <- data.frame(
model = c("by_subj", "by_items"),
BF_01 = vector("numeric", 2)
)
# by-subjects
dist_maximal_subj <- lmer(asin ~ grouping + (1 | by), data = dist_by_subj)
dist_reduced_subj <- lmer(asin ~ 1 + (1 | by), data = dist_by_subj)
dist_bf$BF_01[1] <- exp((BIC(dist_maximal_subj) - BIC(dist_reduced_subj)) / 2)
# by-items
dist_maximal_items <- lmer(asin ~ grouping + (1 | by), data = dist_by_item)
dist_reduced_items <- lmer(asin ~ 1 + (1 | by), data = dist_by_item)
dist_bf$BF_01[2] <- exp((BIC(dist_maximal_items) - BIC(dist_reduced_items)) / 2)
source(here("R-files", "00_helper-functions.R"))
# load packages and helper functions
required_packages <- c(
"here",
"lme4",
"multcomp",
"dplyr",
"tidyr",
"stringr",
"broom",
"ggplot2",
"knitr",
"rmarkdown"
)
## here everything!
lapply(required_packages, library, character.only = TRUE)
source(here("R-files", "00_helper-functions.R"))
filenames <- c(
"01_semantic-results.Rmd",
"02_visual-results.Rmd",
"03_additional-analyses.Rmd"
)
# render documents
for (i in seq_along(filenames)) {
render(filenames[i], output_dir = here("R-files", "output"))
}
for (i in seq_along(filenames)) {
render(filenames[i], output_dir = here("output"))
}
required_packages <- c(
"here",
"lme4",
"multcomp",
"dplyr",
"tidyr",
"stringr",
"broom",
"ggplot2",
"knitr",
"rmarkdown"
)
lapply(required_packages, library, character.only = TRUE)
source(here("R-files", "00_helper-functions.R"))
